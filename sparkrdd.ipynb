{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIADS 516: Homework 2\n",
    "Version 2.0.20201020.1\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Spark RDD API to analyze text\n",
    "Data are from \n",
    "https://www.kaggle.com/nzalake52/new-york-times-articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "1. To gain familiarity with PySpark\n",
    "2. To learn the basics of the Spark RDD API\n",
    "3. To practice solving a real-world problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project was inspired by an actual event that was experienced by a UMSI student.  This student was applying for a \n",
    "job with a large multi-national corporation (let's call it XYZ, Inc.).  XYZ Inc. was looking for someone who could \n",
    "conduct an analysis of a massive (terabyte-size) text dataset.  They had heard about Spark and planned on investigating it but hadn't yet found someone internally who had the skill set required to tackle the problem.  The UMSI student indicated that they had experience with Spark and could likely handle the task.  The hiring supervisor then provided a non-Spark script and asked the student to demonstrate how that script could be translated to work in a Spark environment.  The student was able to do the conversion and, pending completion of their degree, will have secured a job at XYZ, Inc.\n",
    "\n",
    "This assignment simulates that exact situation.  In this assignment you will take a python-based script that does\n",
    "part-of-speech tagging on a large dataset and convert it, as much as possible, to use a pyspark-based approach.\n",
    "\n",
    "The original script was written by Luke Petschauer and a forked version is available at https://github.com/umsi-data-science/NP_chunking_with_nltk/blob/master/NP_chunking_with_the_NLTK.ipynb. That page provides a detailed explanation of the original code and an excellent overview and justification for the use of\n",
    "part-of-speech tagging and a super-gentle introduction to Natural Language Processing (NLP).  **You should read through and study that notebook before you start this assignment.** The code from the \"Final Code\" section is reproduced in the first code cell below, which you should run.\n",
    "\n",
    "Run and study that cell and review https://github.com/umsi-data-science/NP_chunking_with_nltk/blob/master/NP_chunking_with_the_NLTK.ipynb.  \n",
    "You will be taking a similar approach to analyze news articles from the New York Times using pyspark.  There are two tasks (Task 1 and Task 2) to complete below.\n",
    "**You should create and use a short dataset, based on a small fraction of the complete\n",
    "dataset in your development work, and then when you're happy with your code run it on the complete dataset.**\n",
    "The complete analysis should take about 10 minutes to complete."
   ]
  },
  {
   "source": [
    "two tasks for the homwork, related to analysing news articles.\n",
    "\n",
    "\n",
    "\n",
    "tips:\n",
    "create and use a short dataset based on a small fraction of the complete dataset\n",
    "when you're happy with your code, run it on the complete dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required nltk and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/macarthur/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('book') # NOTE: this should be unnecessary for Coursera image (should be preloaded)\n",
    "import re\n",
    "import pprint\n",
    "from nltk import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next cell is the original (non-Spark) script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Stellar pitching', 'afloat', 'first half', 'last season', 'encore', 'pennant-winning season', 'lineup', 'pitching', 'thin', 'pitching', 's game', 'pitching', 'anything', '4-2 loss', 'place', 'spot starter', 'deficit', 'lineup', 'starter', 'room', 'ninth inning', 'last-gasp two-run homer', 'reliever', 'streak', 'team']\n"
     ]
    }
   ],
   "source": [
    "# This is the original (non-Spark) script\n",
    "\n",
    "patterns = \"\"\"\n",
    "    NP: {<JJ>*<NN*>+}\n",
    "    {<JJ>*<NN*><CC>*<NN*>+}\n",
    "    \"\"\"\n",
    "\n",
    "NPChunker = nltk.RegexpParser(patterns)\n",
    "\n",
    "def prepare_text(input):\n",
    "    sentences = nltk.sent_tokenize(input)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    sentences = [NPChunker.parse(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def parsed_text_to_NP(sentences):\n",
    "    nps = []\n",
    "    for sent in sentences:\n",
    "        tree = NPChunker.parse(sent)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP':\n",
    "                t = subtree\n",
    "                t = ' '.join(word for word, tag in t.leaves())\n",
    "                nps.append(t)\n",
    "    return nps\n",
    "\n",
    "\n",
    "def sent_parse(input):\n",
    "    sentences = prepare_text(str(input))\n",
    "    nps = parsed_text_to_NP(sentences)\n",
    "    return nps\n",
    "\n",
    "\n",
    "text_to_be_analyzed = \"\"\"WASHINGTON - Stellar pitching kept the Mets afloat in the first half of last season despite their offensive woes. But they cannot produce an encore of their pennant-winning season if their lineup keeps floundering while their pitching is nicked, bruised and stretched thin.\n",
    "\"We were going to ride our pitching,\" Manager Terry Collins said before Wednesday’s game. \"But we're not riding it right now. We've got as many problems with our pitching as we do anything.\"\n",
    "Wednesday's 4-2 loss to the Washington Nationals was cruel for the already-limping Mets. Pitching in Steven Matz's place, the spot starter Logan Verrett allowed two runs over five innings. But even that was too large a deficit for the Mets' lineup to overcome against Max Scherzer, the Nationals' starter.\n",
    "\"We're not even giving ourselves chances,\" Collins said, adding later, \"We just can’t give our pitchers any room to work.\"\n",
    "The Mets did not score until the ninth inning, when a last-gasp two-run homer by James Loney off Nationals reliever Shawn Kelley snapped a streak of 23 scoreless innings for the team.\"\"\"\n",
    "\n",
    "\n",
    "nps = sent_parse(text_to_be_analyzed)\n",
    "print(nps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('SIADS 516 Homework 2') \\\n",
    "    .getOrCreate() \n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['URL: http://www.nytimes.com/2016/06/30/sports/baseball/washington-nationals-max-scherzer-baffles-mets-completing-a-sweep.html',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "text = sc.textFile('data/nytimes/nytimes_news_articles.txt')\n",
    "# show the first 2 lines of the file\n",
    "text.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_RE = re.compile(r\"\\b[\\w']+\\b\")\n",
    "def pos_tag_counter(line):\n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    return postoks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Create an RDD pipline (i.e. sequence of transformations) that:\n",
    "1. filters out blank lines \n",
    "2. filters out lines starting with 'URL'\n",
    "3. creates a single list (using flatMap) that applies the pos_tag_counter function to each line\n",
    "4. maps each resulting line to show the part of speech (which is the second element returned from the pos_tag_counter)\n",
    "5. converts each resulting line to a pairRDD with POS tags as keys and values of 1\n",
    "6. reduces the resulting RDD by key, adding up all the 1s (like the lecture and lab examples)\n",
    "7. sorts the resulting list by the counts, in descending order.\n",
    "\n",
    "Your output should look something like:\n",
    "```\n",
    "[('NN', 5628),\n",
    " ('IN', 4690),\n",
    " ('NNP', 4575),\n",
    " ('DT', 3913),\n",
    " ('JJ', 2550),\n",
    " ('NNS', 2345),\n",
    " ('VBD', 1931),\n",
    " ('RB', 1312),\n",
    " ('PRP', 1227),\n",
    " ('VB', 1170),\n",
    " ('CC', 1086),\n",
    " ('TO', 1043),\n",
    " ('VBN', 935),\n",
    " ('VBG', 895)...\n",
    " ```\n",
    " In other words, the count of each part-of-speech tag sorted in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('NN', 1126487), ('IN', 928880), ('NNP', 853054), ('DT', 761466), ('JJ', 498464)]\n"
     ]
    }
   ],
   "source": [
    "#step 1 and 2:\n",
    "text_filtered = text.filter(lambda x: (len(x)>0) & ('URL' not in x))\n",
    "#step 3 and 4:\n",
    "text_pos = text_filtered.flatMap(lambda x: pos_tag_counter(x)).map(lambda x: x[1])\n",
    "#step 5:\n",
    "text_pos_kv = text_pos.map(lambda x: (x, 1))\n",
    "#step 6 and 7:\n",
    "text_pos_sorted = text_pos_kv.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "res_preview = text_pos_sorted.take(5)\n",
    "print('preview first five elements from results: ', res_preview)\n",
    "res = text_pos_sorted.collect()\n",
    "print('results: ', res)\n",
    "\n",
    "\n",
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the next cell before proceeding to Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJS>*<NN.*>}\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}\n",
    "\"\"\"\n",
    "\n",
    "  \n",
    "def tokenize_chunk_parse(line):\n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "  \n",
    "    toks = nltk.regexp_tokenize(line, TOKEN_RE)\n",
    "    postoks = nltk.tag.pos_tag(toks)\n",
    "\n",
    "    tree = chunker.parse(postoks)\n",
    "\n",
    "    return [term for term in leaves(tree)] \n",
    "  \n",
    "def leaves(tree):\n",
    "    for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "        yield subtree.leaves()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:  Create an RDD pipeline to show the distribution of the length of noun phrases\n",
    "You should wind up with an RDD showing the number of each length of noun phrase.  For example, the following output shows there are 6157 noun phrases of\n",
    "length 1, 1833 of length 2, 654 of length 3, and so on:\n",
    "```\n",
    "[(1, 6157),\n",
    " (2, 1833),\n",
    " (3, 654),\n",
    " (4, 204),\n",
    " (5, 65),\n",
    " (6, 16),\n",
    " (8, 6),\n",
    " (7, 4),\n",
    " (9, 3)]\n",
    "```\n",
    "The steps should be:\n",
    "1. Apply (using flatMap) the ```tokenize_chunk_parse``` function to each line in the ```text``` RDD\n",
    "2. Use map to emit the length of each noun phrase\n",
    "3. Use map to convert each resulting line to a pairRDD with lengths of noun phrases as keys and values of 1\n",
    "4. Reduce the resulting RDD by key, adding up all the 1s (like the lecture and lab examples)\n",
    "5. Sort the resulting list by the counts, in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(5, 415258), (6, 412931), (4, 369253), (7, 355174), (8, 252965)]\n"
     ]
    }
   ],
   "source": [
    "#step 1:\n",
    "text_np = text.flatMap(lambda x: tokenize_chunk_parse(x))\n",
    "#step 2:\n",
    "text_np2 = text_np.map(lambda x: [len(i[0]) for i in x])\n",
    "#step 3:\n",
    "text_np_kv = text_np2.flatMap(lambda x: x).map(lambda x: (x, 1))\n",
    "#step 4 & 5:\n",
    "text_np_sorted = text_np_kv.reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending = False)\n",
    "\n",
    "res2_preview = text_np_sorted.take(5)\n",
    "print('preview first five elements from results: ', res2_preview)\n",
    "res2 = text_np_sorted.collect()\n",
    "print('results: ', res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('study notes': pipenv)",
   "metadata": {
    "interpreter": {
     "hash": "388315bd7c39d924161dfd1987afd1f69797c9f49ac61e660c82770e887ddc23"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "name": "si330wn18hw9",
  "notebookId": 258872427021699
 },
 "nbformat": 4,
 "nbformat_minor": 1
}