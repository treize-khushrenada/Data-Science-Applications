{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIADS 516: Homework 1\n",
    "Version 1.0.20200221.1\n",
    "### Dr. Chris Teplovs, School of Information, University of Michigan\n",
    "<small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a>This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first mrjob script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the following example from the lectures:\n",
    "\n",
    "Note the use of the magic command ```%%file```.  You can use this to write the contents of a cell out to a file, which is what we need to do to use mrjob:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file word_count.py\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "class MRWordFrequencyCount(MRJob):\n",
    "\n",
    "  ### input: self, in_key, in_value\n",
    "  def mapper(self, _, line):\n",
    "    yield \"chars\", len(line)\n",
    "    yield \"words\", len(line.split())\n",
    "    yield \"lines\", 1\n",
    "\n",
    "  ### input: self, in_key from mapper, in_value from mapper\n",
    "  def reducer(self, key, values):\n",
    "    yield key, sum(values)\n",
    "if __name__ == \"__main__\":\n",
    "    MRWordFrequencyCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q1: Explain what each of the yield statements in the above script do.  Provide a list of what the first few iterations through the mapper() step would yield if the script was run against the ```data/gutenberg/short.t1.txt``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first yield statement gives the count of characters. The second yield statements gives the count of words. The third yield statement gives the count of line(s). The last yield statment gives the summarised values for characters, words, and lines, throughout the text file. \n",
    "\n",
    "first 5 iterations:\n",
    "\"chars\"\t69\n",
    "\"words\"\t12\n",
    "\"lines\"\t1\n",
    "\n",
    "\"chars\"\t0\n",
    "\"words\"\t0\n",
    "\"lines\"\t1\n",
    "\n",
    "\"chars\"\t64\n",
    "\"words\"\t14\n",
    "\"lines\"\t1\n",
    "\n",
    "\"chars\"\t68\n",
    "\"words\"\t12\n",
    "\"lines\"\t1\n",
    "\n",
    "\"chars\"\t69\n",
    "\"words\"\t12\n",
    "\"lines\"\t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the output of running the script against that same file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20210308.040651.802760\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20210308.040651.802760/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20210308.040651.802760/output...\n",
      "\"chars\"\t10653\n",
      "\"words\"\t1822\n",
      "\"lines\"\t200\n",
      "Removing temp directory /tmp/word_count.jovyan.20210308.040651.802760...\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py data/gutenberg/short.t1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2.  Repeat the above cell using the the works of William Shakespeare text file (data/gutenberg/t8.shakespeare.txt).  Provide an interpretation of the output (don't overthink this -- just demonstrate that you can find the relevant information in the output).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/word_count.jovyan.20210308.040717.136616\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/word_count.jovyan.20210308.040717.136616/output\n",
      "Streaming final output from /tmp/word_count.jovyan.20210308.040717.136616/output...\n",
      "\"chars\"\t5333743\n",
      "\"words\"\t901325\n",
      "\"lines\"\t124456\n",
      "Removing temp directory /tmp/word_count.jovyan.20210308.040717.136616...\n"
     ]
    }
   ],
   "source": [
    "!python word_count.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are 124456 lines in the text file, with 901325 words and 5333743 characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert your interpretation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's look at a slightly more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting most_used_word.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        # optimization: sum the words we've seen so far\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q3: Explain what the yield statements in the  above script do.  Provide a list of what the first few iterations through the steps would yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Assuming there's only the first line in the file, according to Sean from Slack discussion: https://umich-mads.slack.com/archives/G01PV0P9CQG/p1614894175075100)\n",
    "\n",
    "The yield statement under mapper_get_words gives every word in the text file except the ones in the stopwords list, in lower case format, with '1' as the count of the word itself:\n",
    "\n",
    "\"project\"\t1\n",
    "\"gutenberg's\"\t1\n",
    "\"year\"\t1\n",
    "\"2889\"\t1\n",
    "\"jules\"\t1\n",
    "\"verne\"\t1\n",
    "\"michel\"\t1\n",
    "\"verne\"\t1\n",
    "\n",
    "\n",
    "The yield statment under combiner_count_words takes the results above and gives word-counts pairs which sums the count of the same words occur on the same line:\n",
    "\n",
    "\"2889\"\t1\n",
    "\"gutenberg's\"\t1\n",
    "\"jules\"\t1\n",
    "\"michel\"\t1\n",
    "\"project\"\t1\n",
    "\"verne\"\t2\n",
    "\"year\"\t1\n",
    "\n",
    "The yield statement under reducer_count_words takes the word-counts pairs above and gives the word-counts pairs of the same words across the text file.\n",
    "\n",
    "null\t[1, \"2889\"]\n",
    "null\t[1, \"michel\"]\n",
    "null\t[1, \"project\"]\n",
    "null\t[1, \"jules\"]\n",
    "null\t[1, \"year\"]\n",
    "null\t[2, \"verne\"]\n",
    "null\t[1, \"gutenberg's\"]\n",
    "\n",
    "The yield statment under reducer_find_max_word finds the word-counts pair from the aobve pairs.\n",
    "\n",
    "2\t\"verne\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the file against data/gutenberg/short.t1.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word.jovyan.20210308.060219.287286\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word.jovyan.20210308.060219.287286/output\n",
      "Streaming final output from /tmp/most_used_word.jovyan.20210308.060219.287286/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /tmp/most_used_word.jovyan.20210308.060219.287286...\n",
      "7.887237787246704\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q4: Run the above script on the Shakespeare text file.  What answer do you get?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max word:\n",
    "5479\t\"thou\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q5: What is the impact of removing the combiner from the above code in terms of efficiency?  What does that suggest?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing most_used_word2.py\n"
     ]
    }
   ],
   "source": [
    "%%file most_used_word2.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") # any whitespace or apostrophe, used to split lines below\n",
    "\n",
    "\n",
    "class MRMostUsedWord(MRJob):\n",
    "    STOPWORDS = {'i', 'we', 'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than'}\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                  \n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if word.lower() not in self.STOPWORDS:\n",
    "                yield (word.lower(), 1)\n",
    "\n",
    "   \n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        # send all (num_occurrences, word) pairs to the same reducer.\n",
    "        # num_occurrences is used so we can easily use Python's max() function.\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    # discard the key; it is just None\n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        # each item of word_count_pairs is (count, word),\n",
    "        # so yielding one results in key=counts, value=word\n",
    "        yield max(word_count_pairs)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostUsedWord.run()\n",
    "    end = time.time()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/most_used_word2.jovyan.20210308.060125.083941\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "job output is in /tmp/most_used_word2.jovyan.20210308.060125.083941/output\n",
      "Streaming final output from /tmp/most_used_word2.jovyan.20210308.060125.083941/output...\n",
      "5479\t\"thou\"\n",
      "Removing temp directory /tmp/most_used_word2.jovyan.20210308.060125.083941...\n",
      "6.140726566314697\n"
     ]
    }
   ],
   "source": [
    "!python most_used_word2.py data/gutenberg/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example of Shakespeare text file above, there is a decrease in the execution time of the python code (from ~8 seconds to ~6 seconds). it suggests that more computation power would be consumed if we added an extra mrjob function, as there is extra handling for each iteration. It should suggest linear time complexity (O(n))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q6: Write an mrjob script that finds the 10 words that have the most syllables from the t5.churchill.txt file.  Interpret your results.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting top10_most_syllables.py\n"
     ]
    }
   ],
   "source": [
    "%%file top10_most_syllables.py\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import syllables\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\") \n",
    "    \n",
    "class MRMostSyllables(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words),\n",
    "            MRStep(reducer=self.reducer),\n",
    "            MRStep(reducer=self.reducer_find_top_word)\n",
    "        ]\n",
    "\n",
    "     \n",
    "    def mapper_get_words (self, _, line):\n",
    "        # yield each word in the line\n",
    "        for word in WORD_RE.findall(line):\n",
    "            word = word.lower()\n",
    "            syllables_count = syllables.estimate(word)\n",
    "            yield (syllables_count, word), 1\n",
    "        \n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        yield None,(key, sum(values))        \n",
    "    \n",
    "    def reducer_find_top_word(self, _, key_val_pair):\n",
    "        l =sorted(key_val_pair, key = lambda x: (x[0][0], x[1]), reverse = True)\n",
    "        for i in l[:10]:\n",
    "            yield i\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import time\n",
    "    start = time.time()\n",
    "    MRMostSyllables.run()\n",
    "    end = time.time()\n",
    "    print(end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/top10_most_syllables.jovyan.20210308.145823.925381\n",
      "Running step 1 of 3...\n",
      "Running step 2 of 3...\n",
      "Running step 3 of 3...\n",
      "job output is in /tmp/top10_most_syllables.jovyan.20210308.145823.925381/output\n",
      "Streaming final output from /tmp/top10_most_syllables.jovyan.20210308.145823.925381/output...\n",
      "[8, \"incommunicability\"]\t1\n",
      "[8, \"overcapitalization\"]\t1\n",
      "[7, \"apologetically\"]\t29\n",
      "[7, \"characteristically\"]\t22\n",
      "[7, \"incompatibility\"]\t6\n",
      "[7, \"inaccessibility\"]\t4\n",
      "[7, \"imperturbability\"]\t3\n",
      "[7, \"inconsiderately\"]\t3\n",
      "[7, \"uncommunicative\"]\t3\n",
      "[7, \"disproportionately\"]\t2\n",
      "Removing temp directory /tmp/top10_most_syllables.jovyan.20210308.145823.925381...\n",
      "75.42973279953003\n"
     ]
    }
   ],
   "source": [
    "!python top10_most_syllables.py data/gutenberg/t5.churchill.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the results we can interpret that the top 10 most syllable words in t5.churchill.txt file are:\n",
    "1. incommunicability, 8 syllables, occured 1 time in the text file\n",
    "2. overcapitalization, 8 syllables, occured 1 time in the text file\n",
    "3. apologetically, 7 syllables, occured 29 times in the text file\n",
    "4. characteristically, 7 syllables, occured 22 times in the text file\n",
    "5. incompatibility, 7 syllables, occured 6 times in the text file\n",
    "6. inaccessibility, 7 syllables, occured 4 times in the text file\n",
    "7. imperturbability, 7 syllables, occured 3 times in the text file\n",
    "8. inconsiderately, 7 syllables, occured 3 times in the text file\n",
    "9. uncommunicative, 7 syllables, occured 3 times in the text file\n",
    "10. disproportionately, 7 syllables, occured 2 times in the text file\n",
    "\n",
    "because there are many words with 7 syllables, we use word frequency as a secondary factor to determine the top10 list."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
